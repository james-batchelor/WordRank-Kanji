{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1fe8a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "import hashlib\n",
    "import html\n",
    "import xml.etree.ElementTree as ET\n",
    "from io import BytesIO\n",
    "from pathlib import Path\n",
    "from collections import Counter, defaultdict\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import regex as re\n",
    "import jaconv\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from pydub import AudioSegment\n",
    "from google.cloud import texttospeech\n",
    "from google.api_core import exceptions\n",
    "from sudachipy import tokenizer, dictionary\n",
    "\n",
    "from consts import JMDICT_ENTITIES, JLPT_LEVELS, TRANSLATION_OVERRIDES, READING_OVERRIDES, \\\n",
    "    KANJI_OVERRIDES, TOP_KANJI_ORDER, SIMILAR_KANJI_GROUPS, DONT_DROP_ME"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8498d345",
   "metadata": {},
   "source": [
    "### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc5f0272",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x.y.z:\n",
    "#  - x: Increment for breaking changes. This means changing the names or order of fields in the \"WordRank Kanji\" note\n",
    "#       type (but not appending new ones actually), or deleting card types. Prefer to deprecate/hide rather than delete.\n",
    "#  - y: Increment for new features and non-breaking big changes, ie adding new processed data (eg adding pitch accent),\n",
    "#       probably reordering, etc.\n",
    "#  - z: Increment for data cleanups, typo fixes etc.\n",
    "semver = (1,0,0)\n",
    "\n",
    "# A lighter faster run to check things work as intended\n",
    "test_mode = False\n",
    "  # How many sentences to use from each corpora in test mode\n",
    "sentence_limit = 10000\n",
    "\n",
    "home_dir = Path.cwd()\n",
    "data_dir = home_dir / \"data\"\n",
    "\n",
    "# Corpora files (each should be rows of plain Japanese sentences). Also, the relative weighting for contributions of\n",
    "# each kanji/vocab to the total \"count\" from the given corpora (doesn't have to add up to anything).\n",
    "CORPORA = {\n",
    "    \"jesc\": {\"filepath\": data_dir / \"JESC_jp.txt\", \"weight\": 40},\n",
    "    \"wl_web\": {\"filepath\": data_dir / \"wl_web.txt\", \"weight\": 35},\n",
    "    \"wl_news\": {\"filepath\": data_dir / \"wl_news.txt\", \"weight\": 16},\n",
    "    \"wl_wiki\": {\"filepath\": data_dir / \"wl_wiki.txt\", \"weight\": 9},\n",
    "}\n",
    "\n",
    "# How many vocab lines to include on the cards\n",
    "top_n_vocab = 15"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a65194f7",
   "metadata": {},
   "source": [
    "### Stage 1: Utils and setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f90651",
   "metadata": {},
   "outputs": [],
   "source": [
    "SMALL_KANA = set(\"ゃゅょぁぃぅぇぉャュョァィゥェォ\")\n",
    "\n",
    "\n",
    "def is_kanji(c):\n",
    "    \"\"\"Return True if the character is in the CJK Unified block, Ext A or 々.\"\"\"\n",
    "    return '\\u4e00' <= c <= '\\u9fff' or '\\u3400' <= c <= '\\u4dbf' or c == '\\u3005'\n",
    "\n",
    "\n",
    "def is_japanese_string(s, keep_numbers=True):\n",
    "    \"\"\"\n",
    "    True if string contains only Japanese scripts: Kanji, Hiragana, Katakana, prolonged vowel mark ー.\n",
    "    Filters out Chinese or mixed foreign strings.\n",
    "    \"\"\"\n",
    "    if not isinstance(s, str) or not s:\n",
    "        return False\n",
    "    if keep_numbers:\n",
    "        pattern = r'^[\\p{Han}\\p{Hiragana}\\p{Katakana}ー・0-9０-９]+$'\n",
    "    else:\n",
    "        pattern = r'[\\p{Han}\\p{Hiragana}\\p{Katakana}ー]+'\n",
    "    return re.fullmatch(pattern, s) is not None\n",
    "\n",
    "\n",
    "def is_hiragana_string(s):\n",
    "    return isinstance(s, str) and all('\\u3041' <= c <= '\\u309F' for c in s)\n",
    "\n",
    "\n",
    "def is_kana(ch):\n",
    "    \"\"\"Return True if ch is hira, kata, prolonged sound mark, middle dot.\"\"\"\n",
    "    return (\n",
    "        '\\u3040' <= ch <= '\\u309f' or\n",
    "        '\\u30a0' <= ch <= '\\u30ff' or\n",
    "        ch in ('ー', '・')\n",
    "    )\n",
    "\n",
    "\n",
    "version = f\"v{semver[0]}.{semver[1]}.{semver[2]}\"\n",
    "out_dir = home_dir / \"out\" / version\n",
    "\n",
    "home_dir.mkdir(parents=True, exist_ok=True)\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "wavs_dir = data_dir / \"wavs\"\n",
    "mp3s_dir = data_dir / \"mp3s\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd497255",
   "metadata": {},
   "source": [
    "### Stage 2: Parse JP dictionaries to usable dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65454ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_jmdict_to_df(filepath, jmdict_entities):\n",
    "    \"\"\"\n",
    "    Parse JMdict XML into a DataFrame.\n",
    "\n",
    "    Returns DataFrame with columns:\n",
    "        lemma, jmdict_reading, reading_kata, translations, is_godan, is_ichidan, is_vt, is_it, is_suru, is_irregular,\n",
    "        is_onomatopoeic, is_mimetic, is_uk\n",
    "    \"\"\"\n",
    "    # Invert entity mapping\n",
    "    human_to_codes = {}\n",
    "    for code, text in jmdict_entities.items():\n",
    "        if not text:\n",
    "            continue\n",
    "        key = text.strip().lower()\n",
    "        human_to_codes.setdefault(key, set()).add(code)\n",
    "\n",
    "    tree = ET.parse(filepath)\n",
    "    root = tree.getroot()\n",
    "    rows = []\n",
    "\n",
    "    BAD_MISC = {\"arch\", \"obs\", \"rare\", \"poet\"}\n",
    "\n",
    "    def add_codes(token, target):\n",
    "        \"\"\"Attach grammatical/misc codes from POS or misc tags.\"\"\"\n",
    "        if not token:\n",
    "            return\n",
    "        text = token.strip().lower()\n",
    "        if not text:\n",
    "            return\n",
    "\n",
    "        raw = text.strip(\"&;\")\n",
    "        if raw in jmdict_entities:\n",
    "            target.add(raw)\n",
    "            return\n",
    "\n",
    "        if text in human_to_codes:\n",
    "            target.update(human_to_codes[text])\n",
    "            return\n",
    "\n",
    "        if \"ichidan\" in text or \"v1\" in text:\n",
    "            target.add(\"v1\")\n",
    "        if \"godan\" in text or \"v5\" in text:\n",
    "            target.add(\"v5\")\n",
    "        if \"transitive\" in text:\n",
    "            target.add(\"vt\")\n",
    "        if \"intransitive\" in text:\n",
    "            target.add(\"vi\")\n",
    "        if \"suru\" in text or \"verbal noun\" in text:\n",
    "            target.add(\"vs\")\n",
    "        if \"irreg\" in text:\n",
    "            target.add(\"irreg\")\n",
    "        if \"uk\" in text or \"kana alone\" in text:\n",
    "            target.add(\"uk\")\n",
    "        if \"onomatopoeic\" in text or \"sound\" in text:\n",
    "            target.add(\"onm\")\n",
    "        if \"mimetic\" in text or \"phenomenon\" in text or \"sensory\" in text:\n",
    "            target.add(\"mim\")\n",
    "\n",
    "    # === Main JMdict parse ===\n",
    "    for entry in root.findall(\"entry\"):\n",
    "        lemmas = [k.text for k in entry.findall(\"k_ele/keb\")] or [None]\n",
    "\n",
    "        # process each sense separately\n",
    "        senses = []\n",
    "        for sense in entry.findall(\"sense\"):\n",
    "            glosses = [g.text.strip().rstrip('. ') for g in sense.findall(\"gloss\") if g.text]\n",
    "            pos_tags = [p.text.strip() for p in sense.findall(\"pos\") if p.text]\n",
    "            misc_tags = [m.text.strip() for m in sense.findall(\"misc\") if m.text]\n",
    "\n",
    "            pos_codes, misc_codes = set(), set()\n",
    "            for p in pos_tags:\n",
    "                add_codes(p, pos_codes)\n",
    "            for m in misc_tags:\n",
    "                add_codes(m, misc_codes)\n",
    "\n",
    "            is_godan        = any(code.startswith(\"v5\") for code in pos_codes)\n",
    "            is_ichidan      = any(code.startswith(\"v1\") for code in pos_codes)\n",
    "            is_vt           = \"vt\" in pos_codes\n",
    "            is_it           = \"vi\" in pos_codes\n",
    "            is_suru         = any(code.startswith(\"vs\") for code in pos_codes)\n",
    "            is_uk           = \"uk\" in pos_codes or \"uk\" in misc_codes\n",
    "            is_irregular    = \"irreg\" in misc_codes or any(code in (\"v5ri\", \"vn\", \"vr\") for code in pos_codes)\n",
    "            is_onomatopoeic = \"onm\" in pos_codes or \"onm\" in misc_codes\n",
    "            is_mimetic      = \"mim\" in pos_codes or \"mim\" in misc_codes\n",
    "\n",
    "            senses.append({\n",
    "                \"glosses\": glosses,\n",
    "                \"pos_codes\": pos_codes,\n",
    "                \"misc_codes\": misc_codes,\n",
    "                \"is_godan\": is_godan,\n",
    "                \"is_ichidan\": is_ichidan,\n",
    "                \"is_vt\": is_vt,\n",
    "                \"is_it\": is_it,\n",
    "                \"is_suru\": is_suru,\n",
    "                \"is_uk\": is_uk,\n",
    "                \"is_irregular\": is_irregular,\n",
    "                \"is_onomatopoeic\": is_onomatopoeic,\n",
    "                \"is_mimetic\": is_mimetic,\n",
    "            })\n",
    "        \n",
    "        # group senses by POS set\n",
    "        senses_by_pos = defaultdict(list)\n",
    "        for s in senses:\n",
    "            # use a stable key for the POS set\n",
    "            pos_key = tuple(sorted(s[\"pos_codes\"]))  # e.g. ('adj-na', 'n') or ('n',) or ('adv',)\n",
    "            senses_by_pos[pos_key].append(s)\n",
    "\n",
    "        # now for each POS group, merge the senses\n",
    "        merged_senses = []\n",
    "        for pos_key, group in senses_by_pos.items():\n",
    "            # drop bad/misc ones if we have any good ones\n",
    "            good = [s for s in group if not (set(s[\"misc_codes\"]) & BAD_MISC)]\n",
    "            if good:\n",
    "                group = good\n",
    "\n",
    "            # round-robin gloss merge\n",
    "            gloss_lists = [s[\"glosses\"] for s in group]\n",
    "            merged_glosses = []\n",
    "            i = 0\n",
    "            # find max length\n",
    "            max_len = max(len(gl) for gl in gloss_lists) if gloss_lists else 0\n",
    "            while i < max_len:\n",
    "                for gl in gloss_lists:\n",
    "                    if i < len(gl):\n",
    "                        merged_glosses.append(gl[i])\n",
    "                i += 1\n",
    "\n",
    "            # merge flags (OR)\n",
    "            merged_flags = {\n",
    "                \"is_godan\": any(s[\"is_godan\"] for s in group),\n",
    "                \"is_ichidan\": any(s[\"is_ichidan\"] for s in group),\n",
    "                \"is_vt\": any(s[\"is_vt\"] for s in group),\n",
    "                \"is_it\": any(s[\"is_it\"] for s in group),\n",
    "                \"is_suru\": any(s[\"is_suru\"] for s in group),\n",
    "                \"is_irregular\": any(s[\"is_irregular\"] for s in group),\n",
    "                \"is_uk\": any(s[\"is_uk\"] for s in group),\n",
    "                \"is_onomatopoeic\": any(s[\"is_onomatopoeic\"] for s in group),\n",
    "                \"is_mimetic\": any(s[\"is_mimetic\"] for s in group),\n",
    "            }\n",
    "\n",
    "            merged_senses.append({\n",
    "                \"pos_codes\": list(pos_key),\n",
    "                \"misc_codes\": sorted(set().union(*[s[\"misc_codes\"] for s in group])),\n",
    "                \"glosses\": merged_glosses,\n",
    "                **merged_flags,\n",
    "            })\n",
    "\n",
    "        # emit rows for each (reading, lemma, merged_pos_group)\n",
    "        for r_ele in entry.findall(\"r_ele\"):\n",
    "            reading = r_ele.findtext(\"reb\")\n",
    "            if not reading:\n",
    "                continue\n",
    "            re_restrs = [r.text for r in r_ele.findall(\"re_restr\")]\n",
    "            if re_restrs:\n",
    "                linked_lemmas = [k for k in lemmas if k in re_restrs]\n",
    "            else:\n",
    "                linked_lemmas = lemmas\n",
    "\n",
    "            for lemma in linked_lemmas:\n",
    "                for ms in merged_senses:\n",
    "                    reading_kata = jaconv.hira2kata(reading) if isinstance(reading, str) else None\n",
    "                    rows.append({\n",
    "                        \"lemma\": lemma,\n",
    "                        \"jmdict_reading\": reading,\n",
    "                        \"reading_kata\": reading_kata,\n",
    "                        \"sense_glosses\": ms[\"glosses\"],\n",
    "                        \"sense_pos\": ms[\"pos_codes\"],\n",
    "                        \"sense_misc\": ms[\"misc_codes\"],\n",
    "                        \"is_godan\": ms[\"is_godan\"],\n",
    "                        \"is_ichidan\": ms[\"is_ichidan\"],\n",
    "                        \"is_vt\": ms[\"is_vt\"],\n",
    "                        \"is_it\": ms[\"is_it\"],\n",
    "                        \"is_suru\": ms[\"is_suru\"],\n",
    "                        \"is_irregular\": ms[\"is_irregular\"],\n",
    "                        \"is_uk\": ms[\"is_uk\"],\n",
    "                        \"is_onomatopoeic\": ms[\"is_onomatopoeic\"],\n",
    "                        \"is_mimetic\": ms[\"is_mimetic\"],\n",
    "                    })\n",
    "\n",
    "    # JMdict sometimes lists both hiragana and katakana readings for the same word, which causes\n",
    "    # downstream duplicates, so keep only one row per (lemma, reading_kata)\n",
    "    tmp = {}\n",
    "    for data in rows:\n",
    "        reading = data[\"jmdict_reading\"]\n",
    "        reading_kata = jaconv.hira2kata(reading) if isinstance(reading, str) else None\n",
    "        key = (data[\"lemma\"], reading_kata, tuple(data[\"sense_glosses\"]))\n",
    "        # Prefer the row which was the hirigana form\n",
    "        if key not in tmp or is_hiragana_string(reading):\n",
    "            tmp[key] = {**data, \"reading_kata\": reading_kata}    \n",
    "            \n",
    "    return pd.DataFrame(tmp.values())\n",
    "\n",
    "\n",
    "def parse_kanjidic2_to_df(filepath):\n",
    "    \"\"\"\n",
    "    Parse KANJIDIC2 XML into a DataFrame.\n",
    "\n",
    "    Returns DataFrame with columns:\n",
    "        kanji, radical_id, jlpt_level, jouyou_grade, stroke_count, on_readings, kun_readings, meaning\n",
    "    \"\"\"\n",
    "    tree = ET.parse(filepath)\n",
    "    root = tree.getroot()\n",
    "    rows = []\n",
    "\n",
    "    for char in root.findall(\"character\"):\n",
    "        literal = char.findtext(\"literal\")\n",
    "\n",
    "        misc = char.find(\"misc\")\n",
    "        jlpt = JLPT_LEVELS.get(literal)\n",
    "        jouyou = misc.findtext(\"grade\") if misc is not None else None\n",
    "        if jouyou:\n",
    "            # 1-6 are grades 1-6 (~1000 kanji), 7 doesn't exist, 8 is high school (another ~1000),\n",
    "            # 9/10 aren't to do with jouyou\n",
    "            jouyou = int(jouyou)\n",
    "            jouyou = jouyou if jouyou and jouyou <= 8 else None\n",
    "\n",
    "        stroke_counts = [int(s.text) for s in misc.findall(\"stroke_count\")] if misc is not None else []\n",
    "        stroke_count = min(stroke_counts) if stroke_counts else None\n",
    "\n",
    "        rad = char.find(\"radical/rad_value[@rad_type='classical']\")\n",
    "        radical = int(rad.text) if rad is not None else None\n",
    "\n",
    "        rm = char.find(\"reading_meaning/rmgroup\")\n",
    "        on_yomi, kun_yomi, meanings = [], [], []\n",
    "        if rm is not None:\n",
    "            for r in rm.findall(\"reading\"):\n",
    "                if r.attrib.get(\"r_type\") == \"ja_on\":\n",
    "                    on_yomi.append(r.text)\n",
    "                elif r.attrib.get(\"r_type\") == \"ja_kun\":\n",
    "                    kun_yomi.append(r.text)\n",
    "            for m in rm.findall(\"meaning\"):\n",
    "                if m.attrib.get(\"m_lang\") is None:\n",
    "                    if \"radical\" not in m.text and \"kokuji\" not in m.text:\n",
    "                        meanings.append(m.text)\n",
    "\n",
    "        rows.append({\n",
    "            \"kanji\": literal,\n",
    "            \"radical_id\": radical,\n",
    "            \"jlpt_level\": int(jlpt) if jlpt else None,\n",
    "            \"jouyou_grade\": jouyou if jouyou else None,\n",
    "            \"stroke_count\": stroke_count,\n",
    "            \"on_readings\": on_yomi,\n",
    "            \"kun_readings\": kun_yomi,\n",
    "            \"meaning\": meanings\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "def parse_pitch_accents_to_df(filepath):\n",
    "    rows = []\n",
    "    with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            surface, reading, accents = line.split(\"\\t\")\n",
    "            \n",
    "            reading_hira = jaconv.kata2hira(reading)\n",
    "\n",
    "            rows.append({\n",
    "                \"surface\": surface,\n",
    "                \"reading\": reading_hira,\n",
    "                \"accents\": accents,\n",
    "            })\n",
    "\n",
    "    return pd.DataFrame(rows).drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c032864",
   "metadata": {},
   "outputs": [],
   "source": [
    "kanjidic2_df = parse_kanjidic2_to_df(data_dir / \"kanjidic2.xml\")\n",
    "jmdict_df = parse_jmdict_to_df(data_dir / \"JMdict_e.xml\", JMDICT_ENTITIES)\n",
    "pitch_df = parse_pitch_accents_to_df(data_dir / \"pitch_accents.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055804d0",
   "metadata": {},
   "source": [
    "### Stage 3: Get the base kanji and vocab count df's\n",
    "Base meaning that we've done the frequency analysis and very minimal cleaning, but haven't started any real manual touch-ups yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b18cd86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_kanji_df(corpora):\n",
    "    \"\"\"\n",
    "    Count kanji frequencies per corpus.\n",
    "\n",
    "    Returns DataFrame with columns:\n",
    "        kanji, count_<corpus>\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    for corpus, cfg in corpora.items():\n",
    "        i = 0\n",
    "        print(f\"Starting kanji_df extraction for {corpus}\")\n",
    "        counter = Counter()\n",
    "        with open(cfg[\"filepath\"], \"r\", encoding=\"utf-8\") as f:\n",
    "            for sentence in f:\n",
    "                if test_mode:\n",
    "                    i += 1\n",
    "                    if i > sentence_limit:\n",
    "                        break\n",
    "                for ch in sentence:\n",
    "                    if is_kanji(ch):\n",
    "                        counter[ch] += 1\n",
    "        for kanji, count in counter.items():\n",
    "            rows.append({\"kanji\": kanji, f\"count_{corpus}\": count})\n",
    "        print(f\"Total kanji so far: {len(rows)}\")\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "\n",
    "    count_cols = [c for c in df.columns if c.startswith(\"count_\")]\n",
    "    df = df.groupby(\"kanji\", as_index=False)[count_cols].sum()\n",
    "    for c in count_cols:\n",
    "        df[c] = df[c].astype(int)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_vocab_df(corpora):\n",
    "    \"\"\"\n",
    "    Build vocab_df from corpora using Sudachi\n",
    "\n",
    "    Returns DataFrame with columns:\n",
    "        lemma, reading_kata, pos, conj, count_<corpus>\n",
    "    \"\"\"\n",
    "    tokenizer_obj = dictionary.Dictionary().create()\n",
    "    tokenizer_mode = tokenizer.Tokenizer.SplitMode.C\n",
    "\n",
    "    def strip_numbers(s):\n",
    "        if not isinstance(s, str):\n",
    "            return s\n",
    "        s = jaconv.z2h(s, digit=True, ascii=False, kana=False)\n",
    "        return re.sub(r'[0-9]+', '', s)\n",
    "\n",
    "    rows = []\n",
    "    for corpus, cfg in corpora.items():\n",
    "        i = 0\n",
    "        print(f\"Starting vocab_df extraction for {corpus}\")\n",
    "        counter = Counter()\n",
    "        with open(cfg[\"filepath\"], \"r\", encoding=\"utf-8\") as f:\n",
    "            for sentence in f:\n",
    "                if test_mode:\n",
    "                    i += 1\n",
    "                    if i > sentence_limit:\n",
    "                        break\n",
    "                for token in tokenizer_obj.tokenize(sentence, tokenizer_mode):\n",
    "                    # Normalise out numeric digits (人[hito], 1人[hito], 2人[hito] are all counted as 人[hito])\n",
    "                    lemma = strip_numbers(token.dictionary_form())\n",
    "                    reading_kata = strip_numbers(token.reading_form())  # always in katakana from Sudachi\n",
    "                    pos = token.part_of_speech()[0]\n",
    "                    conj = token.part_of_speech()[4]\n",
    "\n",
    "                    if pos in [\"助詞\", \"助動詞\", \"補助記号\", \"記号\"]:\n",
    "                        # particles, verb endings, symbols, punctuation\n",
    "                        continue\n",
    "\n",
    "                    counter[(lemma, reading_kata, pos, conj)] += 1\n",
    "\n",
    "        for (lemma, reading_kata, pos, conj), count in counter.items():\n",
    "            rows.append({\n",
    "                \"lemma\": lemma,\n",
    "                \"reading_kata\": reading_kata,\n",
    "                \"pos\": pos,\n",
    "                \"conj\": conj,\n",
    "                f\"count_{corpus}\": count\n",
    "            })\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "\n",
    "    count_cols = [c for c in df.columns if c.startswith(\"count_\")]\n",
    "    df = df.groupby([\"lemma\", \"reading_kata\", \"pos\", \"conj\"], as_index=False)[count_cols].sum()\n",
    "    for c in count_cols:\n",
    "        df[c] = df[c].astype(int)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def filter_non_japanese_vocab(vocab_df):\n",
    "    keep = []\n",
    "    dropped = []\n",
    "    for _, row in vocab_df.iterrows():\n",
    "        lemma = row[\"lemma\"]\n",
    "        if is_japanese_string(lemma):\n",
    "            keep.append(row)\n",
    "        else:\n",
    "            dropped.append(lemma)\n",
    "    clean_df = pd.DataFrame(keep).reset_index(drop=True)\n",
    "    return clean_df, dropped\n",
    "\n",
    "\n",
    "def merge_jmdict_to_vocab(vocab_df, jmdict_df, n_translations=3):\n",
    "    \"\"\"\n",
    "    Merge JMdict info into vocab_df on (lemma, reading_kata).\n",
    "\n",
    "    Adds columns:\n",
    "        jmdict_reading, translations, translation,\n",
    "        is_godan, is_ichidan, is_vt, is_it, is_suru\n",
    "    \"\"\"\n",
    "\n",
    "    # Simple POS bucket matcher\n",
    "    def sudachi_pos_bucket(pos):\n",
    "        if not isinstance(pos, str):\n",
    "            return None\n",
    "        if pos.startswith(\"動詞\"):\n",
    "            return \"verb\"\n",
    "        if pos.startswith(\"形容詞\"):\n",
    "            return \"adj-i\"\n",
    "        if pos.startswith(\"形容動詞\"):\n",
    "            return \"adj-na\"\n",
    "        if pos.startswith(\"名詞\"):\n",
    "            return \"noun\"\n",
    "        if pos.startswith(\"副詞\"):\n",
    "            return \"adv\"\n",
    "        if pos.startswith(\"形状詞\"):\n",
    "            return \"noun\"\n",
    "        if pos.startswith(\"連体詞\"):\n",
    "            return \"adj-na\"\n",
    "        return None\n",
    "\n",
    "    JM_POS_GROUPS = {\n",
    "        \"verb\": {\"v1\", \"v5\", \"vs\", \"vi\", \"vt\"},\n",
    "        \"adj-i\": {\"adj-i\"},\n",
    "        \"adj-na\": {\"adj-na\"},\n",
    "        \"noun\": {\"n\", \"n-adv\", \"n-t\", \"n-pref\", \"n-suf\"},\n",
    "    }\n",
    "\n",
    "    def pick_sense_for_row(senses, sudachi_pos):\n",
    "        \"\"\"Return gloss list from JMdict sense matching Sudachi POS best.\"\"\"\n",
    "        bucket = sudachi_pos_bucket(sudachi_pos)\n",
    "        if not isinstance(senses, list):\n",
    "            return []\n",
    "\n",
    "        best = None\n",
    "        for s in senses:\n",
    "            codes = set(s.get(\"sense_pos\", []))\n",
    "            if bucket and bucket in JM_POS_GROUPS:\n",
    "                if codes & JM_POS_GROUPS[bucket]:\n",
    "                    return s.get(\"sense_glosses\", [])\n",
    "            if best is None:\n",
    "                best = s\n",
    "        return best.get(\"sense_glosses\", []) if best else []\n",
    "\n",
    "    # 1) group all senses per (lemma, reading_kata)\n",
    "    j_grouped = (\n",
    "        jmdict_df\n",
    "        .groupby([\"lemma\", \"reading_kata\"], dropna=False)\n",
    "        .apply(lambda g: g.to_dict(orient=\"records\"), include_groups=False)\n",
    "        .rename(\"senses\")\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    # 2) ALSO get one of the constant jmdict_reading per (lemma, reading_kata)\n",
    "    j_reading_map = (\n",
    "        jmdict_df\n",
    "        .groupby([\"lemma\", \"reading_kata\"], dropna=False)[\"jmdict_reading\"]\n",
    "        .first()\n",
    "        .rename(\"jmdict_reading\")\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    # merge both into vocab\n",
    "    merged = vocab_df.merge(j_grouped, on=[\"lemma\", \"reading_kata\"], how=\"left\")\n",
    "    merged = merged.merge(j_reading_map, on=[\"lemma\", \"reading_kata\"], how=\"left\")\n",
    "\n",
    "    # 3) pick the right sense for each Sudachi row\n",
    "    translations = []\n",
    "    for _, row in merged.iterrows():\n",
    "        glosses = pick_sense_for_row(row.get(\"senses\"), row.get(\"pos\"))\n",
    "        translations.append(\"; \".join(glosses[:n_translations]) if glosses else \"\")\n",
    "    merged[\"translation\"] = translations\n",
    "\n",
    "    # Boolean flags: OR them (from all sense rows)\n",
    "    bool_cols = [\"is_godan\", \"is_ichidan\", \"is_vt\", \"is_it\", \"is_suru\"]\n",
    "    for col in bool_cols:\n",
    "        if col in jmdict_df:\n",
    "            m = (jmdict_df.groupby([\"lemma\", \"reading_kata\"], dropna=False)[col]\n",
    "                 .any()\n",
    "                 .rename(col)\n",
    "                 .reset_index())\n",
    "            merged = merged.drop(columns=col, errors=\"ignore\").merge(m, on=[\"lemma\", \"reading_kata\"], how=\"left\")\n",
    "            merged[col] = merged[col].astype(\"boolean\").fillna(False)\n",
    "\n",
    "    return merged\n",
    "\n",
    "\n",
    "def merge_kanjidic2_to_kanji(kanji_df, kanjidic2_df):\n",
    "    \"\"\"\n",
    "    Merge KANJIDIC2 data into kanji_df on 'kanji', keeping only matches.\n",
    "\n",
    "    Returns DataFrame with columns:\n",
    "        kanji, count_<corpus>, radical_id, jlpt_level, jouyou_grade, stroke_count, on_readings, kun_readings, meaning\n",
    "    \"\"\"\n",
    "    merged = kanji_df.merge(kanjidic2_df, on=\"kanji\", how=\"inner\")\n",
    "    return merged\n",
    "\n",
    "\n",
    "def get_kanji_used_in_vocab(vocab_df):\n",
    "    \"\"\"\n",
    "    Return the set of kanji that appear in at least one REAL Japanese vocab item.\n",
    "    \"Real\" = lemma is a string AND we have a non-empty JMdict-backed translation.\n",
    "    We do NOT check KANJIDIC2 here; this is purely usage-based.\n",
    "    \"\"\"\n",
    "    vocab_kanji = set()\n",
    "\n",
    "    for _, row in vocab_df.iterrows():\n",
    "        lemma = row.get(\"lemma\")\n",
    "        tl = row.get(\"translation\", \"\")\n",
    "\n",
    "        if not isinstance(lemma, str):\n",
    "            continue\n",
    "        if not isinstance(tl, str) or not tl.strip():\n",
    "            continue\n",
    "\n",
    "        for ch in lemma:\n",
    "            if is_kanji(ch):\n",
    "                vocab_kanji.add(ch)\n",
    "\n",
    "    return vocab_kanji\n",
    "\n",
    "\n",
    "def filter_non_japanese_kanji(kanji_df, vocab_kanji):\n",
    "    \"\"\"\n",
    "    Keep a kanji if ANY of the following is true:\n",
    "      - it has a Jōyō grade (jouyou_grade not null)\n",
    "      - it has a JLPT level (jlpt_level not null)\n",
    "      - it appears in at least one real vocab item (vocab_kanji)\n",
    "\n",
    "    Everything else gets dropped (Chinese-only, historical garbage, etc.).\n",
    "    \"\"\"\n",
    "\n",
    "    def is_good(row):\n",
    "        return (\n",
    "            pd.notna(row.get(\"jouyou_grade\")) or\n",
    "            pd.notna(row.get(\"jlpt_level\")) or\n",
    "            (row[\"kanji\"] in vocab_kanji)\n",
    "        )\n",
    "\n",
    "    filtered_kanji_df = kanji_df[kanji_df.apply(is_good, axis=1)].reset_index(drop=True)\n",
    "    return filtered_kanji_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22bf70be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The bulk of computation time is in this cell, so skip rerunning it if wanted\n",
    "reload_base_dfs = False\n",
    "if reload_base_dfs:\n",
    "    base_kanji_df = pd.read_parquet(out_dir / \"base_kanji_df.parquet\")\n",
    "    base_vocab_df = pd.read_parquet(out_dir / \"base_vocab_df.parquet\")\n",
    "\n",
    "else:\n",
    "    base_vocab_df = get_vocab_df(CORPORA)\n",
    "    base_vocab_df, dropped_vocab = filter_non_japanese_vocab(base_vocab_df)\n",
    "    base_vocab_df = merge_jmdict_to_vocab(base_vocab_df, jmdict_df)\n",
    "\n",
    "    base_kanji_df = get_kanji_df(CORPORA)\n",
    "    base_kanji_df = merge_kanjidic2_to_kanji(base_kanji_df, kanjidic2_df)\n",
    "    vocab_kanji = get_kanji_used_in_vocab(base_vocab_df)\n",
    "    base_kanji_df = filter_non_japanese_kanji(base_kanji_df, vocab_kanji)\n",
    "\n",
    "    base_kanji_df.to_parquet(out_dir / \"base_kanji_df.parquet\", index=False)\n",
    "    base_vocab_df.to_parquet(out_dir / \"base_vocab_df.parquet\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0238b680",
   "metadata": {},
   "source": [
    "### Stage 4: Clean up the dfs\n",
    "We rely on the dictionaries for most things which aren't raw frequency counts. But partly due to the specifics of this\n",
    "use-case, and partly due to the way this use-case is written, what we get isn't completely what would be ideal. So clean\n",
    "up the vocab and kanji dfs, and apply manual corrections defined in consts.py."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8adca28b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_bad_reading_fixes(vocab_df, fixes):\n",
    "    df = vocab_df.copy()\n",
    "    for (lemma, bad_reading), new_reading in fixes.items():\n",
    "        mask = (df[\"lemma\"] == lemma) & (df[\"jmdict_reading\"] == bad_reading)\n",
    "        if not mask.any():\n",
    "            continue\n",
    "        df.loc[mask, \"jmdict_reading\"] = new_reading\n",
    "        df.loc[mask, \"reading_kata\"] = jaconv.hira2kata(new_reading)\n",
    "        \n",
    "    return df\n",
    "\n",
    "\n",
    "def apply_translation_overrides(vocab_df, overrides):\n",
    "    df = vocab_df.copy()\n",
    "\n",
    "    for (lemma, reading), new_tl in overrides.items():\n",
    "        mask = (df[\"lemma\"] == lemma) & (df[\"reading_kata\"] == reading)\n",
    "        df.loc[mask, \"translation\"] = new_tl\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def collapse_vocab_reading_variants(vocab_df):\n",
    "    \"\"\"\n",
    "    Sudachi can sometimes output multiple rows for the same lemma+POS+conj, differing only in reading_kata\n",
    "    (e.g. 行く: イク, イコ, イキ, etc.).\n",
    "\n",
    "    - If no JMdict-backed rows exist in the group, leave the group untouched.\n",
    "    - If one or more rows have jmdict_reading (i.e. valid dictionary reading exists):\n",
    "      - Choose the JMdict-backed row with the highest count (first count_* column) (expect which corpus it is to not\n",
    "        matter).\n",
    "      - Sum ALL rows' counts (even non-JMdict weird Sudachi variants) into that row.\n",
    "      - Drop all other rows.\n",
    "    \"\"\"\n",
    "    count_cols = [c for c in vocab_df.columns if c.startswith(\"count_\")]\n",
    "    primary_count = count_cols[0]  # If multiple corpora, choose the first for comparison\n",
    "\n",
    "    collapsed = []\n",
    "\n",
    "    for _, group in vocab_df.groupby([\"lemma\", \"pos\", \"conj\"], sort=False):\n",
    "        jmdict_rows = group[group[\"translation\"].astype(bool)]\n",
    "\n",
    "        if jmdict_rows.empty:\n",
    "            # No dictionary reading, keep all variants\n",
    "            collapsed.append(group)\n",
    "            continue\n",
    "\n",
    "        # Pick the JMdict-backed row with the highest primary count\n",
    "        keep = jmdict_rows.sort_values(primary_count, ascending=False).iloc[0].copy()\n",
    "\n",
    "        # Sum counts from ALL variants (dict and non-dict)\n",
    "        for c in count_cols:\n",
    "            keep[c] = group[c].sum()\n",
    "\n",
    "        collapsed.append(keep.to_frame().T)\n",
    "\n",
    "    out = pd.concat(collapsed, ignore_index=True)\n",
    "    \n",
    "    # Hacky fix for 両: merge all 両 rows (any POS) into the JMdict-backed 名詞 row if present\n",
    "    # (the noun form (an old unit of currency) was getting badly over-represented)\n",
    "    mask_all_ryo = out[\"lemma\"] == \"両\"\n",
    "    if mask_all_ryo.any():\n",
    "        noun_mask = mask_all_ryo & (out[\"pos\"] == \"名詞\") & out[\"translation\"].notna() & out[\"translation\"].astype(bool)\n",
    "        if noun_mask.any():\n",
    "            noun_idx = out[noun_mask].index[0]\n",
    "            for c in count_cols:\n",
    "                out.at[noun_idx, c] = out.loc[mask_all_ryo, c].sum()\n",
    "            # drop the other 両 rows\n",
    "            drop_idx = out[mask_all_ryo].index.difference([noun_idx])\n",
    "            if len(drop_idx):\n",
    "                out = out.drop(drop_idx).reset_index(drop=True)\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "def collapse_vocab_by_definition(vocab_df):\n",
    "    \"\"\"\n",
    "    Second-pass dedupe.\n",
    "\n",
    "    After Sudachi variant collapsing, we can still have rows like:\n",
    "        lemma=何, translation=\"what\"\n",
    "    twice, because Sudachi gave different POS buckets, but our JMdict sense-picker resolved both to the same English\n",
    "    gloss.\n",
    "\n",
    "    Strategy:\n",
    "      - build a *normalized translation key* (order-insensitive, lowercase)\n",
    "      - group by (lemma, norm_translation_key)\n",
    "      - sum all count_* columns\n",
    "      - `OR` all boolean JMdict flags\n",
    "      - pick a representative row for the other columns\n",
    "    \"\"\"\n",
    "\n",
    "    # 1) which columns are counts / booleans\n",
    "    count_cols = [c for c in vocab_df.columns if c.startswith(\"count_\")]\n",
    "    bool_cols = [\"is_godan\", \"is_ichidan\", \"is_vt\", \"is_it\", \"is_suru\"]\n",
    "\n",
    "    def norm_translation(s: str) -> str:\n",
    "        if not isinstance(s, str):\n",
    "            return \"\"\n",
    "        # split on ';', normalise, sort so \"what; how many\" == \"how many; what\"\n",
    "        parts = [p.strip().lower() for p in s.split(\";\") if p.strip()]\n",
    "        if not parts:\n",
    "            return \"\"\n",
    "        parts = sorted(set(parts))\n",
    "        return \"; \".join(parts)\n",
    "\n",
    "    # 2) build the key column\n",
    "    vocab_df = vocab_df.copy()\n",
    "    vocab_df[\"__norm_tr__\"] = vocab_df[\"translation\"].apply(norm_translation)\n",
    "\n",
    "    rows = []\n",
    "    for (_, _), grp in vocab_df.groupby([\"lemma\", \"__norm_tr__\"], dropna=False):\n",
    "        # don't merge rows that have *no* translation – those are often junk / unlinked\n",
    "        if grp[\"__norm_tr__\"].iloc[0] == \"\":\n",
    "            rows.append(grp.drop(columns=\"__norm_tr__\"))\n",
    "            continue\n",
    "\n",
    "        # sum counts across the group\n",
    "        summed_counts = {c: grp[c].sum() for c in count_cols}\n",
    "\n",
    "        # OR the boolean flags\n",
    "        bool_vals = {b: bool(grp[b].any()) for b in bool_cols if b in grp}\n",
    "\n",
    "        # pick representative row:\n",
    "        # - prefer one that actually had a JMdict reading\n",
    "        # - else the one with the highest total count\n",
    "        def total_count(row):\n",
    "            return sum(row[c] for c in count_cols)\n",
    "\n",
    "        rep = None\n",
    "        with_jmdict = grp[grp[\"jmdict_reading\"].notna() & (grp[\"jmdict_reading\"] != \"\")]\n",
    "        if not with_jmdict.empty:\n",
    "            rep = with_jmdict.loc[with_jmdict.apply(total_count, axis=1).idxmax()].copy()\n",
    "        else:\n",
    "            rep = grp.loc[grp.apply(total_count, axis=1).idxmax()].copy()\n",
    "\n",
    "        # now write the merged values back on the representative\n",
    "        for c, v in summed_counts.items():\n",
    "            rep[c] = v\n",
    "        for b, v in bool_vals.items():\n",
    "            rep[b] = v\n",
    "\n",
    "        # pos / conj: pick the most common in the group,\n",
    "        # but only overwrite if there was genuine variation\n",
    "        if grp[\"pos\"].nunique() > 1:\n",
    "            rep[\"pos\"] = grp[\"pos\"].mode().iloc[0]\n",
    "        if \"conj\" in grp and grp[\"conj\"].nunique() > 1:\n",
    "            rep[\"conj\"] = grp[\"conj\"].mode().iloc[0]\n",
    "\n",
    "        rows.append(rep.to_frame().T)\n",
    "\n",
    "    out = pd.concat(rows, ignore_index=True)\n",
    "    return out\n",
    "\n",
    "\n",
    "def merge_pitch_accent(vocab_df, pitch_df):\n",
    "    \"\"\"\n",
    "    Attach a single chosen pitch accent per vocab row.\n",
    "\n",
    "    Matching key:\n",
    "        (lemma, reading_hira)\n",
    "    where reading_hira is jmdict_reading if present, else reading_kata converted to hiragana.\n",
    "\n",
    "    New column:\n",
    "        vocab_df[\"pitch_accent\"]\n",
    "    \"\"\"\n",
    "\n",
    "    # --- helper: map Sudachi POS -> short POS code(s) used in pitch data ---\n",
    "    def sudachi_pos_to_pitch_codes(pos: str):\n",
    "        if not isinstance(pos, str):\n",
    "            return []\n",
    "        if pos.startswith(\"名詞\"):\n",
    "            return [\"名\"]\n",
    "        if pos.startswith(\"副詞\"):\n",
    "            return [\"副\"]\n",
    "        if pos.startswith(\"形容動詞\"):\n",
    "            return [\"形動\"]\n",
    "        if pos.startswith(\"感動詞\"):\n",
    "            return [\"感\"]\n",
    "        if pos.startswith(\"代名詞\"):\n",
    "            return [\"代\"]\n",
    "        return []\n",
    "\n",
    "    # --- helper: parse an accents string into POS-specific + default lists ---\n",
    "    def parse_pos_accent(accent_str: str):\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "            pos_map: dict[str, list[int]]  e.g. {'副': [0], '名': [3,4,0]}\n",
    "            default_accents: list[int]      for numbers not under any (POS)\n",
    "        \"\"\"\n",
    "        pos_map = {}\n",
    "        default_accents = []\n",
    "\n",
    "        if not isinstance(accent_str, str):\n",
    "            return pos_map, default_accents\n",
    "\n",
    "        s = accent_str.strip()\n",
    "        if not s:\n",
    "            return pos_map, default_accents\n",
    "\n",
    "        # Fast path: no POS markers -> simple list\n",
    "        if \"(\" not in s:\n",
    "            nums = []\n",
    "            for t in s.split(\",\"):\n",
    "                t = t.strip()\n",
    "                if t.isdigit():\n",
    "                    nums.append(int(t))\n",
    "            default_accents.extend(nums)\n",
    "            return pos_map, default_accents\n",
    "\n",
    "        # General path: with POS markers\n",
    "        tokens = [t.strip() for t in s.split(\",\") if t.strip()]\n",
    "        cur_poses = None  # list[str] or None\n",
    "\n",
    "        for tok in tokens:\n",
    "            # There might be multiple \"(POS)\" markers in one token, but in your\n",
    "            # data they appear cleanly per token; still, be tolerant.\n",
    "            while tok.startswith(\"(\"):\n",
    "                close = tok.find(\")\")\n",
    "                if close == -1:\n",
    "                    break\n",
    "                pos_part = tok[1:close]  # e.g. \"副\" or \"名;形動\"\n",
    "                pos_codes = [p.strip() for p in pos_part.split(\";\") if p.strip()]\n",
    "                cur_poses = pos_codes or None\n",
    "                tok = tok[close + 1:].strip()\n",
    "\n",
    "            if not tok:\n",
    "                continue\n",
    "            if not tok.isdigit():\n",
    "                continue\n",
    "\n",
    "            val = int(tok)\n",
    "\n",
    "            if cur_poses:\n",
    "                for p in cur_poses:\n",
    "                    pos_map.setdefault(p, []).append(val)\n",
    "            else:\n",
    "                default_accents.append(val)\n",
    "\n",
    "        return pos_map, default_accents\n",
    "\n",
    "    p = pitch_df.copy()\n",
    "    p[\"reading_hira\"] = p[\"reading\"].astype(str).map(lambda s: jaconv.kata2hira(s))\n",
    "    p[\"surface\"] = p[\"surface\"].astype(str)\n",
    "    p = p[(p[\"surface\"] != \"\") & (p[\"reading_hira\"] != \"\")]\n",
    "    p = p.drop_duplicates(subset=[\"surface\", \"reading_hira\"], keep=\"first\")\n",
    "    \n",
    "    pitch_map = {\n",
    "        (row[\"surface\"], row[\"reading_hira\"]): row[\"accents\"]\n",
    "        for _, row in p.iterrows()\n",
    "    }\n",
    "\n",
    "    # --- per-row resolver ---\n",
    "    def resolve_pitch_for_row(row):\n",
    "        lemma = row.get(\"lemma\")\n",
    "        if not isinstance(lemma, str) or not lemma:\n",
    "            return pd.NA\n",
    "\n",
    "        # reading in hiragana: prefer jmdict_reading, else Sudachi reading_kata\n",
    "        jr = row.get(\"jmdict_reading\")\n",
    "        if isinstance(jr, str) and jr:\n",
    "            reading_hira = jr\n",
    "        else:\n",
    "            rk = row.get(\"reading_kata\")\n",
    "            if not isinstance(rk, str) or not rk:\n",
    "                return pd.NA\n",
    "            reading_hira = jaconv.kata2hira(rk)\n",
    "\n",
    "        key = (lemma, reading_hira)\n",
    "        accents_str = pitch_map.get(key)\n",
    "        if not isinstance(accents_str, str) or not accents_str.strip():\n",
    "            return pd.NA\n",
    "\n",
    "        pos_map, default_accents = parse_pos_accent(accents_str)\n",
    "        target_codes = sudachi_pos_to_pitch_codes(row.get(\"pos\", \"\"))\n",
    "\n",
    "        # 1) POS-specific match\n",
    "        for code in target_codes:\n",
    "            vals = pos_map.get(code)\n",
    "            if vals:\n",
    "                return vals[0]\n",
    "\n",
    "        # 2) default accents (no POS)\n",
    "        if default_accents:\n",
    "            return default_accents[0]\n",
    "\n",
    "        # 3) fallback: first available POS group\n",
    "        if pos_map:\n",
    "            first_pos = sorted(pos_map.keys())[0]\n",
    "            vals = pos_map[first_pos]\n",
    "            if vals:\n",
    "                return vals[0]\n",
    "\n",
    "        return pd.NA\n",
    "\n",
    "    df = vocab_df.copy()\n",
    "    df[\"pitch_accent\"] = df.apply(resolve_pitch_for_row, axis=1)\n",
    "    df[\"pitch_accent\"] = df[\"pitch_accent\"].astype(\"Int64\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def apply_weights_and_freq_rank(df, corpora):\n",
    "    \"\"\"\n",
    "    Process:\n",
    "      - Normalise each count_<corpus> column by total occurrences in that corpus.\n",
    "      - Multiply each by its corpus weight.\n",
    "      - Sum to get weighted_count.\n",
    "      - Multiply by 1e6 to get more intuitive magnitudes (optional but nice).\n",
    "      - Add rank (index column, 1 = most frequent).\n",
    "\n",
    "    Returns DataFrame with added:\n",
    "        weighted_count, rank\n",
    "    \"\"\"\n",
    "    count_cols = [c for c in df.columns if c.startswith(\"count_\")]\n",
    "\n",
    "    # Sanity check that all corpora are present\n",
    "    corpus_names = [c.replace(\"count_\", \"\") for c in count_cols]\n",
    "    if set(corpus_names) != set(corpora.keys()):\n",
    "        raise ValueError(\"Mismatch between corpora dict and df columns\")\n",
    "\n",
    "    # Normalize within each corpus\n",
    "    normalized = pd.DataFrame(index=df.index)\n",
    "    for col in count_cols:\n",
    "        total = df[col].sum()\n",
    "        if total == 0:\n",
    "            raise ValueError(\"Somehow a corpus has 0 vocab\")\n",
    "        normalized[col] = df[col] / total\n",
    "\n",
    "    # Compute weighted total\n",
    "    total_weight = sum(v[\"weight\"] for v in corpora.values())\n",
    "    weighted_count = pd.Series(0.0, index=df.index)\n",
    "    for col in count_cols:\n",
    "        corpus = col.replace(\"count_\", \"\")\n",
    "        w = corpora[corpus][\"weight\"] / total_weight\n",
    "        weighted_count += normalized[col] * w\n",
    "\n",
    "    if 1:\n",
    "        weighted_count *= 1e6\n",
    "    df[\"weighted_count\"] = weighted_count\n",
    "\n",
    "    df = df.sort_values(\"weighted_count\", ascending=False).reset_index(drop=True)\n",
    "    df[\"freq_rank\"] = df.index + 1\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def apply_kanji_overrides(kanji_df, overrides):\n",
    "    df = kanji_df.copy()\n",
    "    for kanji, new_meaning in overrides.items():\n",
    "        parts = [p.strip() for p in new_meaning.split(\";\") if p.strip()]\n",
    "        idx = df.index[df[\"kanji\"] == kanji]\n",
    "        if not idx.empty:\n",
    "            df.at[idx[0], \"meaning\"] = parts\n",
    "    return df\n",
    "\n",
    "\n",
    "def filter_kanji(kanji_df):\n",
    "    df = kanji_df.copy()\n",
    "\n",
    "    cond_jouyou = df[\"jouyou_grade\"].notna()\n",
    "    cond_jlpt = df[\"jlpt_level\"].notna()\n",
    "    cond_freq = df[\"freq_rank\"] <= 2500\n",
    "    cond_whitelist = df[\"kanji\"].isin(DONT_DROP_ME)\n",
    "\n",
    "    keep = cond_jouyou | cond_jlpt | cond_freq | cond_whitelist\n",
    "    return df[keep].reset_index(drop=True)\n",
    "\n",
    "\n",
    "def apply_manual_ordering(kanji_df, top_kanji_order, kanji_groups):\n",
    "    \"\"\"\n",
    "    Compute a deck 'order' that:\n",
    "      1) Puts TOP_KANJI_ORDER at the very front (in the given order).\n",
    "      2) Then walks the remaining kanji by frequency rank. When the first member of any group appears, insert the whole\n",
    "         group preserving the ordering of the group.\n",
    "      3) Everything else follows in plain frequency order.\n",
    "    Leaves 'freq_rank' untouched (that's true frequency). Adds 'deck_rank'.\n",
    "    \"\"\"\n",
    "    df = kanji_df.copy()\n",
    "    present = set(df[\"kanji\"])\n",
    "\n",
    "    # canonical frequency order\n",
    "    freq_order = list(df.sort_values(\"freq_rank\", ascending=True)[\"kanji\"])\n",
    "\n",
    "    # map a kanji to the group it belongs to\n",
    "    kanji_to_group = {}\n",
    "    for group in (kanji_groups or []):\n",
    "        members = [k for k in group if k in present]\n",
    "        if not members:\n",
    "            continue\n",
    "        for k in members:\n",
    "            if k not in kanji_to_group:\n",
    "                kanji_to_group[k] = members\n",
    "\n",
    "    used = set()\n",
    "    output_seq = []\n",
    "\n",
    "    def emit_group_or_single(item):\n",
    "        if item in used or item not in present:\n",
    "            return\n",
    "        group = kanji_to_group.get(item)\n",
    "        if group:\n",
    "            for m in group:\n",
    "                if m not in used:\n",
    "                    used.add(m)\n",
    "                    output_seq.append(m)\n",
    "        else:\n",
    "            used.add(item)\n",
    "            output_seq.append(item)\n",
    "\n",
    "    # 1) top-order section\n",
    "    for k in (top_kanji_order or []):\n",
    "        emit_group_or_single(k)\n",
    "\n",
    "    # 2) walk by frequency\n",
    "    for k in freq_order:\n",
    "        emit_group_or_single(k)\n",
    "\n",
    "    # 3) safety: ensure all present kanji are included\n",
    "    for k in present:\n",
    "        emit_group_or_single(k)\n",
    "\n",
    "    # assign deck_rank (fast and explicit)\n",
    "    rank_map = {k: i + 1 for i, k in enumerate(output_seq)}\n",
    "    df[\"deck_rank\"] = df[\"kanji\"].map(rank_map)\n",
    "\n",
    "    # return df ordered by the new deck sequence\n",
    "    df = df.set_index(\"kanji\").loc[output_seq].reset_index()\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ddd235",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vocab_df_pipeline(df, pitch_df):\n",
    "    df = apply_bad_reading_fixes(df, READING_OVERRIDES)\n",
    "    df = apply_translation_overrides(df, TRANSLATION_OVERRIDES)\n",
    "    df = collapse_vocab_reading_variants(df)\n",
    "    df = collapse_vocab_by_definition(df)\n",
    "    df = merge_pitch_accent(df, pitch_df)\n",
    "    df = apply_weights_and_freq_rank(df, CORPORA)\n",
    "    return df\n",
    "\n",
    "def kanji_df_pipeline(df):\n",
    "    df = apply_kanji_overrides(df, KANJI_OVERRIDES)\n",
    "    df = apply_weights_and_freq_rank(df, CORPORA)\n",
    "    df = filter_kanji(df)\n",
    "    df = apply_manual_ordering(df, TOP_KANJI_ORDER, SIMILAR_KANJI_GROUPS)\n",
    "    return df\n",
    "\n",
    "vocab_df = vocab_df_pipeline(base_vocab_df, pitch_df)\n",
    "kanji_df = kanji_df_pipeline(base_kanji_df)\n",
    "\n",
    "vocab_df.to_parquet(out_dir / \"vocab_df_end.parquet\", index=False)\n",
    "kanji_df.to_parquet(out_dir / \"kanji_df_end.parquet\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b926e569",
   "metadata": {},
   "source": [
    "### Stage 5: Build the anki notes .tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa6fbfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_mora(reading_hira):\n",
    "    \"\"\"\n",
    "    Count mora in a hiragana string.\n",
    "    - small ゃゅょぁぃぅぇぉゃゅょ + preceding char = 1 mora\n",
    "    - っ counts as 1 mora\n",
    "    \"\"\"\n",
    "    if not isinstance(reading_hira, str):\n",
    "        return 0\n",
    "\n",
    "    mora = 0\n",
    "    i = 0\n",
    "    while i < len(reading_hira):\n",
    "        ch = reading_hira[i]\n",
    "        # small kana: combine with previous but still count as one mora total\n",
    "        if ch in SMALL_KANA:\n",
    "            i += 1\n",
    "            continue\n",
    "        mora += 1\n",
    "        i += 1\n",
    "    return mora\n",
    "\n",
    "\n",
    "def highlight_mora(text, mora_index, span_class=\"pitch-drop\"):\n",
    "    \"\"\"\n",
    "    Wrap the mora at mora_index in <span class=\"pitch-drop\">...</span>.\n",
    "    Small kana are attached to the preceding mora.\n",
    "    \"\"\"\n",
    "    if mora_index is None or mora_index <= 0:\n",
    "        return text\n",
    "\n",
    "    result = []\n",
    "    i = 0\n",
    "    current_mora = 0\n",
    "    n = len(text)\n",
    "\n",
    "    while i < n:\n",
    "        ch = text[i]\n",
    "\n",
    "        if ch in SMALL_KANA:\n",
    "            # belongs to previous mora\n",
    "            result.append(ch)\n",
    "            i += 1\n",
    "            continue\n",
    "\n",
    "        # start of a new mora\n",
    "        current_mora += 1\n",
    "\n",
    "        if current_mora == mora_index:\n",
    "            # include this char and any following small kana\n",
    "            result.append(f'<span class=\"{span_class}\">{ch}')\n",
    "            j = i + 1\n",
    "            while j < n and text[j] in SMALL_KANA:\n",
    "                result.append(text[j])\n",
    "                j += 1\n",
    "            result.append('</span>')\n",
    "            i = j\n",
    "        else:\n",
    "            result.append(ch)\n",
    "            i += 1\n",
    "\n",
    "    return \"\".join(result)\n",
    "\n",
    "\n",
    "def build_furigana_html(lemma, reading, pitch=None):\n",
    "    # 1) normalise reading\n",
    "    if not reading:\n",
    "        reading = \"\"\n",
    "    reading_hira = jaconv.kata2hira(reading)\n",
    "\n",
    "    # --- pitch processing: which mora to highlight? ---\n",
    "    heiban_mora = None\n",
    "    drop_mora = None\n",
    "    drop_class = \"pitch-nakadaka\"  # default\n",
    "    if pitch is not None and not pd.isna(pitch):\n",
    "        p = int(pitch)\n",
    "        total_mora = count_mora(reading_hira)\n",
    "        if p == 0:\n",
    "            if total_mora >= 1:\n",
    "                heiban_mora = 1\n",
    "        elif 1 <= p <= total_mora:\n",
    "            drop_mora = p\n",
    "            if p == 1:\n",
    "                drop_class = \"pitch-atamadaka\"\n",
    "            elif p == total_mora:\n",
    "                drop_class = \"pitch-odaka\"\n",
    "            else:\n",
    "                drop_class = \"pitch-nakadaka\"\n",
    "\n",
    "    # 2) make a normalised copy of the lemma *for logic only*\n",
    "    lemma_norm = (lemma\n",
    "                  .replace(\"ヶ\", \"ケ\")\n",
    "                  .replace(\"ヵ\", \"カ\"))\n",
    "\n",
    "    # 3) segment BOTH strings in parallel, using lemma_norm to decide boundaries\n",
    "    norm_segs = []\n",
    "    orig_segs = []\n",
    "\n",
    "    if not lemma_norm:\n",
    "        return \"\"\n",
    "\n",
    "    buf_norm = lemma_norm[0]\n",
    "    buf_orig = lemma[0]\n",
    "    last_is_kanji = is_kanji(lemma_norm[0])\n",
    "\n",
    "    for idx in range(1, len(lemma_norm)):\n",
    "        ch_norm = lemma_norm[idx]\n",
    "        ch_orig = lemma[idx]\n",
    "        now_is_kanji = is_kanji(ch_norm)\n",
    "\n",
    "        if now_is_kanji == last_is_kanji:\n",
    "            buf_norm += ch_norm\n",
    "            buf_orig += ch_orig\n",
    "        else:\n",
    "            norm_segs.append(buf_norm)\n",
    "            orig_segs.append(buf_orig)\n",
    "            buf_norm = ch_norm\n",
    "            buf_orig = ch_orig\n",
    "            last_is_kanji = now_is_kanji\n",
    "\n",
    "    norm_segs.append(buf_norm)\n",
    "    orig_segs.append(buf_orig)\n",
    "\n",
    "    # from here on, use norm_segs for alignment, orig_segs for output\n",
    "    r_idx = 0\n",
    "    mora_pos = 0  # global mora index consumed so far\n",
    "    out_parts = []\n",
    "    hira_segs = [jaconv.kata2hira(s) for s in norm_segs]\n",
    "\n",
    "    for i, seg_norm in enumerate(norm_segs):\n",
    "        seg_orig = orig_segs[i]\n",
    "        seg_hira = hira_segs[i]\n",
    "        is_kanji_seg = is_kanji(seg_norm[0])\n",
    "\n",
    "        # -------- KANA SEGMENT --------\n",
    "        if not is_kanji_seg:\n",
    "            if reading_hira[r_idx:].startswith(seg_hira):\n",
    "                seg_mora = count_mora(seg_hira)\n",
    "\n",
    "                # decide which mora (if any) to highlight in this segment\n",
    "                local_idx = None\n",
    "                span_class = drop_class\n",
    "\n",
    "                # drop mora?\n",
    "                if drop_mora is not None and (mora_pos < drop_mora <= mora_pos + seg_mora):\n",
    "                    local_idx = drop_mora - mora_pos\n",
    "                    span_class = drop_class\n",
    "\n",
    "                # heiban first mora?\n",
    "                elif heiban_mora is not None and (mora_pos < heiban_mora <= mora_pos + seg_mora):\n",
    "                    local_idx = heiban_mora - mora_pos\n",
    "                    span_class = \"pitch-heiban\"\n",
    "\n",
    "                if local_idx is not None:\n",
    "                    seg_out = highlight_mora(seg_orig, local_idx, span_class=span_class)\n",
    "                else:\n",
    "                    seg_out = seg_orig\n",
    "\n",
    "                r_idx += len(seg_hira)\n",
    "                mora_pos += seg_mora\n",
    "            else:\n",
    "                # reading alignment failed. Fall back, no highlighting here\n",
    "                seg_out = seg_orig\n",
    "\n",
    "            out_parts.append(seg_out)\n",
    "            continue\n",
    "\n",
    "        # -------- KANJI SEGMENT --------\n",
    "        # find next kana anchor\n",
    "        anchor_hira = \"\"\n",
    "        for j in range(i + 1, len(norm_segs)):\n",
    "            if not is_kanji(norm_segs[j][0]):\n",
    "                anchor_hira = hira_segs[j]\n",
    "                break\n",
    "\n",
    "        remain = reading_hira[r_idx:]\n",
    "        if anchor_hira:\n",
    "            p = None\n",
    "            for cand_p in range(1, len(remain) + 1):\n",
    "                if remain[cand_p:].startswith(anchor_hira):\n",
    "                    p = cand_p\n",
    "                    break\n",
    "            if p is None:\n",
    "                p = len(remain)\n",
    "            furikana = remain[:p]\n",
    "            r_idx += p\n",
    "        else:\n",
    "            furikana = remain\n",
    "            r_idx = len(reading_hira)\n",
    "        \n",
    "        seg_mora = count_mora(furikana)\n",
    "\n",
    "        local_idx = None\n",
    "        span_class = drop_class\n",
    "\n",
    "        # drop mora in this segment?\n",
    "        if drop_mora is not None and (mora_pos < drop_mora <= mora_pos + seg_mora):\n",
    "            local_idx = drop_mora - mora_pos\n",
    "            span_class = drop_class\n",
    "\n",
    "        # heiban first mora in this segment?\n",
    "        elif heiban_mora is not None and (mora_pos < heiban_mora <= mora_pos + seg_mora):\n",
    "            local_idx = heiban_mora - mora_pos\n",
    "            span_class = \"pitch-heiban\"\n",
    "\n",
    "        if local_idx is not None:\n",
    "            furikana_out = highlight_mora(furikana, local_idx, span_class=span_class)\n",
    "        else:\n",
    "            furikana_out = furikana\n",
    "\n",
    "        mora_pos += seg_mora\n",
    "\n",
    "        if furikana:\n",
    "            out_parts.append(f\"<ruby><rb>{seg_orig}</rb><rt>{furikana_out}</rt></ruby>\")\n",
    "        else:\n",
    "            out_parts.append(seg_orig)\n",
    "\n",
    "    html_str = \"\".join(out_parts)\n",
    "\n",
    "    # final consistency check\n",
    "    reconstructed = \"\".join(c for c in html_str if is_kana(c)).replace(\"ヶ\", \"ケ\").replace(\"ヵ\", \"カ\")\n",
    "    if reading and jaconv.kata2hira(reconstructed) != reading_hira:\n",
    "        # Just dump the whole reading above the whole lemma if we built an inconsistent furigana reading\n",
    "        html_str = f\"<ruby><rb>{lemma}</rb><rt>{reading}</rt></ruby>\"\n",
    "\n",
    "    return html_str\n",
    "\n",
    "\n",
    "def index_vocab_by_kanji(vocab_df):\n",
    "    idx = defaultdict(list)\n",
    "    for i, row in vocab_df.iterrows():\n",
    "        lemma = row[\"lemma\"]\n",
    "        if not isinstance(lemma, str):\n",
    "            continue\n",
    "        for ch in lemma:\n",
    "            if is_kanji(ch):\n",
    "                idx[ch].append(i)\n",
    "    return idx\n",
    "\n",
    "\n",
    "def build_final_kanji_notes(kanji_df, vocab_df, vocab_idx, top_n_vocab=15, mp3s_dir=None):\n",
    "    \"\"\"\n",
    "    Build final notes dataset for Anki export.\n",
    "    Columns:\n",
    "        Kanji (Note ID), Kanji Freq Rank, Kanji Order, Meaning, OnKun, Metadata, Vocab Block, Links, Tags\n",
    "    \"\"\"\n",
    "\n",
    "    rows = []\n",
    "    vocab_used = set()\n",
    "\n",
    "    for _, krow in kanji_df.iterrows():\n",
    "        kanji = krow[\"kanji\"]\n",
    "\n",
    "        # === Frequency index ===\n",
    "        freq_rank = int(krow[\"freq_rank\"]) if \"freq_rank\" in krow and pd.notna(krow[\"freq_rank\"]) else \"\"\n",
    "        deck_rank = int(krow[\"deck_rank\"]) if \"deck_rank\" in krow and pd.notna(krow[\"deck_rank\"]) else freq_rank\n",
    "\n",
    "        # === Meanings ===\n",
    "        meanings = krow.get(\"meaning\", [])\n",
    "        if isinstance(meanings, str):\n",
    "            meanings = [meanings]\n",
    "        meaning_str = \"; \".join(m for m in meanings[:3] if m)\n",
    "        if kanji == \"日\":\n",
    "            meaning_str = \"; \".join(m for m in meanings if m)\n",
    "\n",
    "        # === Metadata ===\n",
    "        jlpt = krow.get(\"jlpt_level\")\n",
    "        jouyou = krow.get(\"jouyou_grade\")\n",
    "        strokes = krow.get(\"stroke_count\")\n",
    "        radical = krow.get(\"radical_id\")\n",
    "\n",
    "        meta_parts = []\n",
    "        if pd.notna(radical):\n",
    "            meta_parts.append(f\"部首{int(radical)}\")\n",
    "        if pd.notna(strokes):\n",
    "            meta_parts.append(f\"画数{int(strokes)}\")\n",
    "        if pd.notna(jouyou):\n",
    "            jouyou = int(jouyou)\n",
    "            if jouyou <= 6:\n",
    "                meta_parts.append(f\"常用小{int(jouyou)}\")\n",
    "            elif jouyou == 8:\n",
    "                meta_parts.append(\"常用中\")\n",
    "            else:\n",
    "                raise ValueError(\"Unexpected jouyou value\")\n",
    "        if pd.notna(jlpt):\n",
    "            meta_parts.append(f\"JLPT N{int(jlpt)}\")\n",
    "\n",
    "        metadata = \"・\".join(meta_parts)\n",
    "\n",
    "        # === On/Kun readings ===\n",
    "        on_yomi = krow.get(\"on_readings\", [])\n",
    "        kun_yomi = krow.get(\"kun_readings\", [])\n",
    "        on_str, kun_str = \"\", \"\"\n",
    "        if isinstance(on_yomi, list) or isinstance(on_yomi, np.ndarray):\n",
    "            on_str = \"・\".join(list(on_yomi))\n",
    "        elif on_yomi:\n",
    "            on_str = str(on_yomi)\n",
    "        if isinstance(kun_yomi, list) or isinstance(kun_yomi, np.ndarray):\n",
    "            kun_str = \"・\".join(list(kun_yomi))\n",
    "        elif kun_yomi:\n",
    "            kun_str = str(kun_yomi)\n",
    "\n",
    "        on_line  = f\"音：　{on_str}\" if on_str else \"　\"\n",
    "        kun_line = f\"訓：　{kun_str}\" if kun_str else \"　\"\n",
    "\n",
    "        onkun_str = f'<div class=\"on\">{on_line}</div><div class=\"kun\">{kun_line}</div>'\n",
    "\n",
    "        # === Vocab breakdown ===\n",
    "        sub = vocab_df.loc[vocab_idx.get(kanji, [])].copy()\n",
    "        if sub.empty:\n",
    "            vocab_block = \"\"\n",
    "        else:\n",
    "            total = sub[\"weighted_count\"].sum()\n",
    "            sub = sub.sort_values(\"freq_rank\")\n",
    "\n",
    "            vocab_lines = []\n",
    "            added_count, low_count = 0, 0\n",
    "            for _, v in sub.iterrows():\n",
    "                if added_count >= top_n_vocab:\n",
    "                    break\n",
    "\n",
    "                lemma = v.get(\"lemma\", \"\")\n",
    "                reading = v.get(\"jmdict_reading\") or v.get(\"reading_kata\") or \"\"\n",
    "                # ensure strings\n",
    "                if not isinstance(lemma, str):\n",
    "                    lemma = \"\"\n",
    "                if not isinstance(reading, str):\n",
    "                    reading = \"\"\n",
    "\n",
    "                translations = v.get(\"translation\", \"\") or \"\"\n",
    "                translations = html.escape(translations)\n",
    "                # Skip empty translation rows (I guess appearing when Sudachipy gives a POS or whatever that doesn't\n",
    "                # match to JMDict)\n",
    "                if not translations.strip():\n",
    "                    continue\n",
    "\n",
    "                percent = int(round(100.0 * v[\"weighted_count\"] / total)) if total > 0 else 0\n",
    "                percent = 99 if percent == 100 else percent  # 100% is a strong claim...\n",
    "                percent_str = f\"{percent}\" if percent >= 1 else \"<1\"\n",
    "\n",
    "                if percent < 1:\n",
    "                    low_count += 1\n",
    "                    if low_count > 3:\n",
    "                        break\n",
    "                \n",
    "                # Colour the percent by frequency band\n",
    "                freq_rank_val = v.get(\"freq_rank\")\n",
    "                if freq_rank_val <= 5000:\n",
    "                    freq_band_class = \"v-percent--core\"\n",
    "                elif freq_rank_val <= 10000:\n",
    "                    freq_band_class = \"v-percent--mid\"\n",
    "                else:\n",
    "                    freq_band_class = \"v-percent--tail\"\n",
    "                \n",
    "                pitch = v.get(\"pitch_accent\", pd.NA)\n",
    "                ruby_html = build_furigana_html(lemma, reading, pitch=pitch)\n",
    "\n",
    "                verb_class, transitivity = \"\", \"\"\n",
    "                if v.get(\"pos\", \"\").startswith(\"動詞\") and not v[\"is_suru\"] and v[\"lemma\"] != \"来る\":\n",
    "                    if v.get(\"is_ichidan\"):\n",
    "                        verb_class = \"一\"\n",
    "                    if v.get(\"is_godan\"):\n",
    "                        verb_class = \"五\"\n",
    "                    if v.get(\"is_it\"):\n",
    "                        transitivity = \"自\"\n",
    "                    if v.get(\"is_vt\"):\n",
    "                        transitivity = \"他\"\n",
    "                if transitivity == \"\" and verb_class:\n",
    "                    transitivity = \"自\"\n",
    "                    transitive_fallbacks = [\"下さる\", \"組む\", \"語りかける\", \"連れ戻す\", \"連れ帰る\", \"持ち去る\", \"超す\", \"恐る\",\n",
    "                        \"越す\", \"盗み出す\", \"削り出す\", \"奪い返す\", \"奪い去る\", \"招き入れる\", \"祈り求める\", \"泣かす\",\n",
    "                        \"繋ぎ合わせる\", \"狂わす\", \"待ち伏せる\", \"恥じる\", \"封切る\", \"叩き潰す\", \"乗り潰す\", \"阻む\", \"蹴り出す\",\n",
    "                        \"垂れ流す\", \"捧ぐ\", \"炊きあげる\"\n",
    "                    ]\n",
    "                    if lemma in transitive_fallbacks:\n",
    "                        transitivity = \"他\"\n",
    "                if verb_class == \"\" and transitivity:\n",
    "                    verb_class = \"五\"\n",
    "                    ichidan_fallbacks = [\"恐る\"]\n",
    "                    if lemma in ichidan_fallbacks:\n",
    "                        verb_class = \"一\"\n",
    "\n",
    "                verb_badge = verb_class + transitivity\n",
    "\n",
    "                if verb_badge:\n",
    "                    translations = f'<span class=\"verb-flags\" data-type=\"{verb_badge}\">{verb_badge}</span> {translations}'\n",
    "\n",
    "                # === Vocab Line html ===\n",
    "                audio_id = None\n",
    "                audio_tag_html = \"\"\n",
    "\n",
    "                if os.path.exists(mp3s_dir) and lemma and reading:\n",
    "                    # 1) unique, namespaced *filename* in media folder\n",
    "                    base = f\"wordrank_kanji__{lemma}_{reading}\"\n",
    "                    audio_filename = f\"{base}.mp3\"\n",
    "                    audio_path = mp3s_dir / audio_filename\n",
    "\n",
    "                    # only add audio if the file actually exists\n",
    "                    if audio_path.is_file():\n",
    "                        # 2) unique, namespaced HTML id (never leaves the card)\n",
    "                        audio_id = f\"wrk_kanji_audio__{lemma}_{reading}\"\n",
    "                        # 3) the Anki [sound:...] tag that refers to the filename\n",
    "                        audio_tag_html = f'[sound:{html.escape(audio_filename)}]'\n",
    "\n",
    "                # visible vocab part; inner span is the actual tap target\n",
    "                if audio_id:\n",
    "                    vocab_part = (\n",
    "                        f'<div class=\"v-vocab\">'\n",
    "                        f'  <span class=\"v-vocab-hit tappable\" data-audio=\"{audio_id}\">{ruby_html}</span>'\n",
    "                        f'</div>'\n",
    "                        f'<div id=\"{audio_id}\" class=\"v-audio\">{audio_tag_html}</div>'\n",
    "                    )\n",
    "                else:\n",
    "                    vocab_part = (\n",
    "                        f'<div class=\"v-vocab\">'\n",
    "                        f'  <span class=\"v-vocab-hit\">{ruby_html}</span>'\n",
    "                        f'</div>'\n",
    "                    )\n",
    "\n",
    "                line_html = (\n",
    "                    f'<div class=\"vocab-line\">'\n",
    "                    f'<div class=\"v-percent {freq_band_class}\">{percent_str}</div>'\n",
    "                    f'{vocab_part}'\n",
    "                    f'<div class=\"v-translations\">{translations}</div>'\n",
    "                    f'</div>'\n",
    "                )\n",
    "\n",
    "                vocab_lines.append(line_html)\n",
    "                added_count += 1\n",
    "                vocab_used.add((lemma, reading, pitch))\n",
    "            \n",
    "            vocab_block = \"\".join(vocab_lines)\n",
    "\n",
    "        # === Tags ===\n",
    "        tags = []\n",
    "\n",
    "        if pd.notna(jlpt):\n",
    "            tags.append(f\"jlpt_n{int(jlpt)}\")\n",
    "        else:\n",
    "            tags.append(\"non_jlpt\")\n",
    "\n",
    "        if pd.notna(jouyou):\n",
    "            if jouyou <= 6:\n",
    "                tags.append(f\"jouyou_sho{int(jouyou)}\")\n",
    "            elif jouyou == 8:\n",
    "                tags.append(\"jouyou_chu\")\n",
    "        else:\n",
    "            tags.append(\"non_jouyou\")\n",
    "\n",
    "        if pd.notna(strokes):\n",
    "            tags.append(f\"strokes_{int(strokes):02d}\")\n",
    "\n",
    "        if pd.notna(radical):\n",
    "            tags.append(f\"radical_{int(radical):03d}\")\n",
    "        \n",
    "        if freq_rank <= 2500:\n",
    "            block = (freq_rank - 1) // 100\n",
    "            start = block * 100 + 1\n",
    "            end = start + 99\n",
    "            tags.append(f\"freq_rank_{start:04d}_to_{end:04d}\")  # eg 0301_to_0400\n",
    "        else:\n",
    "            tags.append(\"freq_rank_2501_plus\")\n",
    "\n",
    "        tag_str = \" \".join(f\"wordrank_kanji::{t}\" for t in tags)\n",
    "\n",
    "        # === Assemble row ===\n",
    "        rows.append({\n",
    "            \"Kanji (Note ID)\": kanji,\n",
    "            \"WordRank Kanji Order\": deck_rank,\n",
    "            \"Frequency Order\": freq_rank,\n",
    "            \"Deck Order (overwrite this one)\": deck_rank,\n",
    "            \"Meaning\": meaning_str,\n",
    "            \"OnKun\": onkun_str,\n",
    "            \"Metadata\": metadata,\n",
    "            \"Vocab Block\": vocab_block,\n",
    "            \"Tags\": tag_str,\n",
    "        })\n",
    "    \n",
    "    assert len(rows) == len(kanji_df)\n",
    "\n",
    "    vocab_used = pd.DataFrame(\n",
    "        list(vocab_used),\n",
    "        columns=[\"lemma\", \"reading\", \"pitch_accent\"]\n",
    "    )\n",
    "\n",
    "    return pd.DataFrame(rows), vocab_used\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc177dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_idx = index_vocab_by_kanji(vocab_df)\n",
    "\n",
    "if test_mode:\n",
    "    kanji_df_ = kanji_df[:100]\n",
    "else:\n",
    "    kanji_df_ = kanji_df.copy()\n",
    "\n",
    "notes_df, vocab_used = build_final_kanji_notes(\n",
    "    kanji_df_, vocab_df, vocab_idx, top_n_vocab=top_n_vocab, mp3s_dir=mp3s_dir\n",
    ")\n",
    "\n",
    "out_path = out_dir / f\"WordRank Kanji{' (test)' if test_mode else ''} {version}.tsv\"\n",
    "notes_df.to_csv(out_path, sep=\"\\t\", index=False, header=False, encoding=\"utf-8\")\n",
    "notes_df.to_parquet(out_dir / \"notes_df_end.parquet\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcbd3cab",
   "metadata": {},
   "source": [
    "### You now have the tsv you can import to Anki\n",
    "Below are stats/checks for the file, and also the audio generation code. That isn't part of the main deck producing\n",
    "pipeline, because what audio to generate depends on what's in your final dfs. But, for the cards to reference the audio,\n",
    "build_final_kanji_notes() needs to know what audio files there are (referencing non-existent files makes anki sad). So\n",
    "if this is your first time running, you'll need to generate the audio below, and then rerun just the notes building bit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95961263",
   "metadata": {},
   "source": [
    "Make a nice plot of what kanjis are in the deck (by jouyou / JLPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a1eeb11",
   "metadata": {},
   "outputs": [],
   "source": [
    "font_path = home_dir / \"fonts\" / \"NotoSansCJK-Regular.ttc\"\n",
    "mpl.font_manager.fontManager.addfont(font_path)\n",
    "mpl.rcParams['font.family'] = mpl.font_manager.FontProperties(fname=font_path).get_name()\n",
    "\n",
    "df = kanji_df.copy()\n",
    "df['jlpt_level']  = df['jlpt_level'].astype('Int64')\n",
    "df['jouyou_grade'] = df['jouyou_grade'].astype('Int64')\n",
    "\n",
    "# --- Y POSITIONS ---\n",
    "jouyou_grades = [1,2,3,4,5,6,8]\n",
    "jouyou_positions = {g: (g if g != 8 else 7) for g in jouyou_grades}\n",
    "non_jouyou_row = 8\n",
    "\n",
    "jlpt_levels = [5,4,3,2,1]\n",
    "# shift JLPT block up by +0.5 to create a visual gap after Non-Jōyō\n",
    "jlpt_positions = {lvl: non_jouyou_row + (5 - lvl) + 1.5 for lvl in jlpt_levels}\n",
    "non_jlpt_row = max(jlpt_positions.values()) + 1\n",
    "\n",
    "# colours\n",
    "jouyou_cmap = plt.cm.viridis_r(np.linspace(0.1, 0.75, len(jouyou_grades)))\n",
    "jlpt_cmap   = plt.cm.magma_r(np.linspace(0.15, 0.6, len(jlpt_levels)))\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "added_labels = set()\n",
    "MS = 10\n",
    "\n",
    "# --- NON−JŌYŌ ---\n",
    "sub = df[df['jouyou_grade'].isna()]\n",
    "if len(sub) > 0:\n",
    "    label = f\"Non-Jōyō ({len(sub)})\"\n",
    "    plt.scatter(sub['freq_rank'], np.full(len(sub), non_jouyou_row),\n",
    "                color=\"grey\", s=MS, alpha=0.25, label=label)\n",
    "    added_labels.add(label)\n",
    "\n",
    "# --- JŌYŌ ---\n",
    "for g, col in zip(jouyou_grades, jouyou_cmap):\n",
    "    subset = df[df['jouyou_grade'] == g]\n",
    "    if len(subset) == 0:\n",
    "        continue\n",
    "\n",
    "    y = jouyou_positions[g]\n",
    "    nice = \"中\" if g == 8 else f\"小{g}\"\n",
    "    label = f\"Jōyō {nice} ({len(subset)})\"\n",
    "\n",
    "    plt.scatter(\n",
    "        subset['freq_rank'], np.full(len(subset), y), color=col, s=MS, alpha=0.25,\n",
    "        label=None if label in added_labels else label\n",
    "    )\n",
    "    added_labels.add(label)\n",
    "\n",
    "# --- JLPT ---\n",
    "for lvl, col in zip(jlpt_levels, jlpt_cmap):\n",
    "    subset = df[df['jlpt_level'] == lvl]\n",
    "    if len(subset) == 0:\n",
    "        continue\n",
    "\n",
    "    y = jlpt_positions[lvl]\n",
    "    label = f\"JLPT N{lvl} ({len(subset)})\"\n",
    "\n",
    "    plt.scatter(\n",
    "        subset['freq_rank'], np.full(len(subset), y), color=col, s=MS, alpha=0.25,\n",
    "        label=None if label in added_labels else label\n",
    "    )\n",
    "    added_labels.add(label)\n",
    "\n",
    "# --- NON-JLPT ---\n",
    "sub = df[df['jlpt_level'].isna()]\n",
    "if len(sub) > 0:\n",
    "    label = f\"Non-JLPT ({len(sub)})\"\n",
    "    plt.scatter(sub['freq_rank'], np.full(len(sub), non_jlpt_row),\n",
    "                color=\"grey\", s=MS, alpha=0.25, label=label)\n",
    "\n",
    "# --- AXES ---\n",
    "plt.xlabel(\"Frequency rank\", fontsize=11)\n",
    "plt.ylabel(\"\")\n",
    "plt.xlim(0, 4250)\n",
    "plt.xticks(np.arange(0, 4001, 500), fontsize=11)\n",
    "\n",
    "# y ticks\n",
    "yticks = []\n",
    "ylabels = []\n",
    "\n",
    "for g in jouyou_grades:\n",
    "    yticks.append(jouyou_positions[g])\n",
    "    ylabels.append(\"Jōyō 中\" if g == 8 else f\"Jōyō 小{g}\")\n",
    "\n",
    "yticks.append(non_jouyou_row)\n",
    "ylabels.append(\"Non-Jōyō\")\n",
    "\n",
    "for lvl in jlpt_levels:\n",
    "    yticks.append(jlpt_positions[lvl])\n",
    "    ylabels.append(f\"JLPT N{lvl}\")\n",
    "\n",
    "yticks.append(non_jlpt_row)\n",
    "ylabels.append(\"Non-JLPT\")\n",
    "\n",
    "plt.yticks(yticks, ylabels, fontsize=11)\n",
    "plt.grid(alpha=0.25)\n",
    "\n",
    "plt.title(f\"WordRank Kanji Inclusions (Total: {len(df)})\", fontsize=15)\n",
    "\n",
    "# ---- LEGEND ORDER ----\n",
    "handles, labels = plt.gca().get_legend_handles_labels()\n",
    "\n",
    "dummy = plt.Line2D([0], [0], marker='o', markersize=8,\n",
    "                   linestyle='', color='white', markerfacecolor='white')\n",
    "\n",
    "handles = [dummy, dummy] + handles\n",
    "labels  = [\"\", \"\"] + labels\n",
    "\n",
    "manual_order = [\n",
    "    0, 1,\n",
    "    labels.index(next(l for l in labels if l.startswith(\"Non-JLPT\"))),\n",
    "    labels.index(next(l for l in labels if l.startswith(\"JLPT N1\"))),\n",
    "    labels.index(next(l for l in labels if l.startswith(\"JLPT N2\"))),\n",
    "    labels.index(next(l for l in labels if l.startswith(\"JLPT N3\"))),\n",
    "    labels.index(next(l for l in labels if l.startswith(\"JLPT N4\"))),\n",
    "    labels.index(next(l for l in labels if l.startswith(\"JLPT N5\"))),\n",
    "\n",
    "    labels.index(next(l for l in labels if l.startswith(\"Non-Jōyō\"))),\n",
    "    labels.index(next(l for l in labels if l.startswith(\"Jōyō 中\"))),\n",
    "    labels.index(next(l for l in labels if l.startswith(\"Jōyō 小6\"))),\n",
    "    labels.index(next(l for l in labels if l.startswith(\"Jōyō 小5\"))),\n",
    "    labels.index(next(l for l in labels if l.startswith(\"Jōyō 小4\"))),\n",
    "    labels.index(next(l for l in labels if l.startswith(\"Jōyō 小3\"))),\n",
    "    labels.index(next(l for l in labels if l.startswith(\"Jōyō 小2\"))),\n",
    "    labels.index(next(l for l in labels if l.startswith(\"Jōyō 小1\")))\n",
    "]\n",
    "\n",
    "leg = plt.legend(\n",
    "    [handles[i] for i in manual_order],\n",
    "    [labels[i]  for i in manual_order],\n",
    "    loc=\"lower right\",\n",
    "    bbox_to_anchor=(0.98, 0.02),\n",
    "    fontsize=10.5,\n",
    "    frameon=True,\n",
    "    ncol=2,\n",
    "    columnspacing=1.3,\n",
    "    labelspacing=0.4,\n",
    "    borderpad=0.7\n",
    ")\n",
    "\n",
    "for h in leg.legend_handles:\n",
    "    try:\n",
    "        h.set_alpha(1.0)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d5a0ed",
   "metadata": {},
   "source": [
    "How much of the top 5k vocab set is missing from the deck (due to the vocab lines cutoff)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10fbbe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_kanji(s):\n",
    "    return isinstance(s, str) and any(is_kanji(ch) for ch in s)\n",
    "\n",
    "def vocab_key_from_row(row):\n",
    "    lemma = row.get(\"lemma\") or \"\"\n",
    "    reading = row.get(\"jmdict_reading\") or row.get(\"reading_kata\") or \"\"\n",
    "    if not isinstance(lemma, str):\n",
    "        lemma = \"\"\n",
    "    if not isinstance(reading, str):\n",
    "        reading = \"\"\n",
    "    return (lemma, reading)\n",
    "\n",
    "# keys from vocab_used (lemma + reading)\n",
    "vocab_used_keys = set(\n",
    "    zip(\n",
    "        vocab_used[\"lemma\"].fillna(\"\").astype(str),\n",
    "        vocab_used[\"reading\"].fillna(\"\").astype(str),\n",
    "    )\n",
    ")\n",
    "\n",
    "vf = vocab_df[pd.notna(vocab_df[\"freq_rank\"])].copy()\n",
    "vf = vf.sort_values(\"freq_rank\")\n",
    "\n",
    "# --- top 5k with kanji in lemma ---\n",
    "top5 = vf[(vf[\"freq_rank\"] <= 5000) & (vf[\"lemma\"].apply(has_kanji))].copy()\n",
    "top5[\"key\"] = top5.apply(vocab_key_from_row, axis=1)\n",
    "\n",
    "missing_top5 = (\n",
    "    top5[~top5[\"key\"].isin(vocab_used_keys)]\n",
    "    .drop_duplicates(\"key\")\n",
    ")\n",
    "\n",
    "cols = [\"lemma\", \"reading_kata\", \"jmdict_reading\", \"translation\", \"pitch_accent\", \"freq_rank\"]\n",
    "missing_top5 = missing_top5[missing_top5[\"translation\"].astype(str) != \"\"][cols]\n",
    "\n",
    "# These are the vocab in top 5k with a translation but never appear in the deck. I think it's acceptable\n",
    "missing_top5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ab92fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "raise Exception(\"Break the auto-run here because this last part is a bit more intentional\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca040f5",
   "metadata": {},
   "source": [
    "Audio generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5532a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate wav readings of the vocabs\n",
    "\n",
    "def iter_mora_chunks(reading_hira: str):\n",
    "    \"\"\"\n",
    "    Split a hiragana string into mora-sized chunks.\n",
    "    Small kana and ー are attached to the preceding character.\n",
    "    \"\"\"\n",
    "    i = 0\n",
    "    n = len(reading_hira)\n",
    "    while i < n:\n",
    "        ch = reading_hira[i]\n",
    "        # base char + any following small kana / long vowel mark\n",
    "        chunk = ch\n",
    "        j = i + 1\n",
    "        while j < n and (reading_hira[j] in SMALL_KANA or reading_hira[j] == \"ー\"):\n",
    "            chunk += reading_hira[j]\n",
    "            j += 1\n",
    "        yield chunk\n",
    "        i = j\n",
    "\n",
    "\n",
    "def build_yomigana_with_pitch(reading: str, pitch) -> str | None:\n",
    "    \"\"\"\n",
    "    reading: kana (hiragana/katakana)\n",
    "    pitch: NHK-style accent index (0 = heiban, 1..N = drop after mora p)\n",
    "    Returns something like '^は!し' or '^あめ!' for alphabet='yomigana'.\n",
    "    \"\"\"\n",
    "    if not isinstance(reading, str) or not reading.strip():\n",
    "        return None\n",
    "\n",
    "    reading_hira = jaconv.kata2hira(reading)\n",
    "    mora_chunks = list(iter_mora_chunks(reading_hira))\n",
    "    total_mora = len(mora_chunks)\n",
    "\n",
    "    try:\n",
    "        p = int(pitch) if pitch is not None else 0\n",
    "    except (TypeError, ValueError):\n",
    "        p = 0\n",
    "\n",
    "    if p < 0 or p > total_mora:\n",
    "        p = 0  # treat garbage as heiban\n",
    "\n",
    "    out = [\"^\"]\n",
    "    for i, chunk in enumerate(mora_chunks, start=1):\n",
    "        out.append(chunk)\n",
    "        if p > 0 and i == p:\n",
    "            out.append(\"!\")\n",
    "\n",
    "    return \"\".join(out)\n",
    "\n",
    "\n",
    "def get_and_save_tts(lemma: str, reading: str, pitch_accent):\n",
    "    \"\"\"\n",
    "    Synthesize audio for a single (lemma, reading, pitch_accent).\n",
    "    Skips if WAV already exists.\n",
    "    \"\"\"\n",
    "    if pd.isna(pitch_accent):\n",
    "        return\n",
    "\n",
    "    wav_path = wavs_dir / f\"wordrank_kanji__{lemma}_{reading}.wav\"\n",
    "    if wav_path.exists():\n",
    "        return\n",
    "    \n",
    "    hash_this = f\"{lemma}+{reading}\"\n",
    "    h = hashlib.sha256(hash_this.encode(\"utf-8\")).digest()\n",
    "    idx = h[0] % len(voices)\n",
    "    voice_name = voices[idx]\n",
    "    voice = texttospeech.VoiceSelectionParams(\n",
    "        language_code=\"ja-JP\",\n",
    "        name=voice_name,\n",
    "    )\n",
    "    audio_config = texttospeech.AudioConfig(\n",
    "        audio_encoding=texttospeech.AudioEncoding.LINEAR16,\n",
    "    )\n",
    "\n",
    "    yomi = build_yomigana_with_pitch(reading, pitch_accent)\n",
    "    safe_yomi = html.escape(yomi)\n",
    "    safe_lemma = html.escape(lemma)\n",
    "    safe_reading = html.escape(reading)\n",
    "\n",
    "    # Couldn't force it to read it the expected way either way...\n",
    "    # ssml = f'<speak><phoneme alphabet=\"yomigana\" ph=\"{safe_yomi}\">{safe_lemma}</phoneme></speak>'\n",
    "    ssml = f'<speak><phoneme alphabet=\"yomigana\" ph=\"{safe_yomi}\">{safe_reading}</phoneme></speak>'\n",
    "\n",
    "    synthesis_input = texttospeech.SynthesisInput(ssml=ssml)\n",
    "\n",
    "    response = None\n",
    "    max_retries = 10\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            response = client.synthesize_speech(\n",
    "                input=synthesis_input,\n",
    "                voice=voice,\n",
    "                audio_config=audio_config,\n",
    "            )\n",
    "            break\n",
    "        except exceptions.ResourceExhausted:\n",
    "            if attempt == max_retries - 1:\n",
    "                raise\n",
    "            sleep_s = (2 ** attempt) + random.random()\n",
    "            time.sleep(sleep_s)\n",
    "\n",
    "    if response is None:\n",
    "        raise RuntimeError(f\"TTS synthesis failed for {lemma}+{reading!r}\")\n",
    "\n",
    "    wav_buf = BytesIO(response.audio_content)\n",
    "    audio = AudioSegment.from_file(wav_buf, format=\"wav\")\n",
    "    audio.export(wav_path, format=\"wav\")\n",
    "\n",
    "\n",
    "def tts_for_row(row):\n",
    "    lemma = row.lemma\n",
    "    reading = row.reading\n",
    "    pitch = row.pitch_accent\n",
    "\n",
    "    try:\n",
    "        get_and_save_tts(lemma, reading, pitch)\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        return f\"{lemma}+{reading}: {e!r}\"\n",
    "\n",
    "\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = str(home_dir / \"sa.json\")\n",
    "client = texttospeech.TextToSpeechClient()\n",
    "\n",
    "voices = [\n",
    "    \"ja-JP-Chirp3-HD-Alnilam\",\n",
    "    \"ja-JP-Chirp3-HD-Aoede\",\n",
    "    \"ja-JP-Chirp3-HD-Callirrhoe\",\n",
    "    \"ja-JP-Chirp3-HD-Fenrir\",\n",
    "    \"ja-JP-Chirp3-HD-Iapetus\",\n",
    "    \"ja-JP-Chirp3-HD-Leda\",\n",
    "    \"ja-JP-Chirp3-HD-Orus\",\n",
    "    \"ja-JP-Chirp3-HD-Rasalgethi\",\n",
    "    \"ja-JP-Chirp3-HD-Sulafat\",\n",
    "]\n",
    "\n",
    "wavs_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "rows = list(vocab_used.itertuples(index=False))\n",
    "print(f\"Total number of vocabs (including ones without pitch): {len(rows)}\")\n",
    "\n",
    "workers = 4\n",
    "failed = []\n",
    "with ThreadPoolExecutor(max_workers=workers) as ex:\n",
    "    futures = [ex.submit(tts_for_row, row) for row in rows]\n",
    "\n",
    "    for n, fut in enumerate(as_completed(futures), start=1):\n",
    "        err = fut.result()\n",
    "        if err is not None:\n",
    "            failed.append(err)\n",
    "        if n % 1000 == 0:\n",
    "            print(f\"Done {n}\")\n",
    "\n",
    "print(f\"Failed {len(failed)} vocabs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a7cdd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up which mp3s we have\n",
    "\n",
    "check_dir = mp3s_dir\n",
    "files = os.listdir(check_dir)\n",
    "print(f\"Total fies: {len(files)}\")\n",
    "\n",
    "# Vocabs (from vocab_used) with a pitch accent\n",
    "valid_vocabs = set()\n",
    "for row in vocab_used.itertuples(index=False):\n",
    "    if pd.isna(row.pitch_accent):\n",
    "        continue\n",
    "    lemma = str(row.lemma)\n",
    "    reading = str(row.reading)\n",
    "    valid_vocabs.add((lemma, reading))\n",
    "print(f\"Should have: {len(valid_vocabs)}\")\n",
    "\n",
    "# Vocabs with an audio file\n",
    "found_vocabs = set()\n",
    "for f in files:\n",
    "    lem, rea = f.split(\"__\")[-1].split(\".\")[0].split(\"_\")\n",
    "    found_vocabs.add((lem, rea))\n",
    "print(f\"Have: {len(found_vocabs)}\")\n",
    "\n",
    "should_del = found_vocabs - valid_vocabs\n",
    "print(f\"Should delete: {len(should_del)}\")\n",
    "\n",
    "missing = valid_vocabs - found_vocabs\n",
    "print(f\"Should generate: {len(missing)}\")\n",
    "\n",
    "if 0:\n",
    "    for f in files:\n",
    "        lem, rea = f.split(\"__\")[-1].split(\".\")[0].split(\"_\")\n",
    "        if (lem, rea) in should_del:\n",
    "            os.remove(check_dir / f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28bc52ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cut wavs and export to mp3\n",
    "\n",
    "def detect_leading_silence(sound, *, silence_thresh_db=-35, chunk_size_ms=10, safety_margin_ms=50):\n",
    "    if len(sound) == 0:\n",
    "        return 0\n",
    "\n",
    "    n_chunks = max(1, int(np.ceil(len(sound) / chunk_size_ms)))\n",
    "\n",
    "    for i in range(n_chunks):\n",
    "        start = i * chunk_size_ms\n",
    "        chunk = sound[start:start + chunk_size_ms]\n",
    "        # use pydub's dBFS (already relative to full scale)\n",
    "        db = chunk.dBFS if chunk.dBFS != float(\"-inf\") else -1000.0\n",
    "        if db > silence_thresh_db:\n",
    "            onset_ms = i * chunk_size_ms\n",
    "            return max(0, onset_ms - safety_margin_ms)\n",
    "\n",
    "    return 0\n",
    "\n",
    "mp3s_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "wav_files = sorted(wavs_dir.glob(\"*.wav\"))\n",
    "wav_files = [data_dir / \"wavs/wordrank_kanji__お買い上げ_おかいあげ.wav\"]\n",
    "print(f\"Found {len(wav_files)} wav files\")\n",
    "\n",
    "failed_mp3 = []\n",
    "\n",
    "for n, wav_path in enumerate(wav_files, start=1):\n",
    "    mp3_path = mp3s_dir / (wav_path.stem + \".mp3\")\n",
    "    if mp3_path.exists():\n",
    "        continue\n",
    "\n",
    "    only_make_missing_files = True\n",
    "    if only_make_missing_files:\n",
    "        lemma, reading = wav_path.stem.split(\"__\")[-1].split(\"_\")\n",
    "        if (lemma, reading) not in missing:\n",
    "            continue\n",
    "\n",
    "    try:\n",
    "        audio = AudioSegment.from_wav(wavs_dir / wav_path)\n",
    "\n",
    "        # strip leading silence\n",
    "        start_trim = detect_leading_silence(audio)\n",
    "        trimmed = audio[start_trim:] if start_trim < len(audio) else audio\n",
    "\n",
    "        # normalise loudness (skip if completely silent)\n",
    "        if np.isinf(trimmed.dBFS):\n",
    "            normalised = trimmed\n",
    "        else:\n",
    "            target_dbfs = -18\n",
    "            gain = target_dbfs - trimmed.dBFS\n",
    "            normalised = trimmed.apply_gain(gain)\n",
    "\n",
    "        # keep it small: mono, modest sample rate, low-ish bitrate\n",
    "        normalised = normalised.set_channels(1).set_frame_rate(24000)\n",
    "        normalised.export(\n",
    "            mp3_path,\n",
    "            format=\"mp3\",\n",
    "            bitrate=\"40k\"  # bump to 64k if you want it nicer\n",
    "        )\n",
    "\n",
    "    except Exception as e:\n",
    "        failed_mp3.append((wav_path.name, repr(e)))\n",
    "\n",
    "    if n % 1000 == 0:\n",
    "        print(f\"Processed {n} wav files\")\n",
    "\n",
    "print(f\"Done. Failed: {len(failed_mp3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54eed81c",
   "metadata": {},
   "source": [
    "If you want to jump back in from a final set of dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab5ee9e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fp = out_dir / \"vocab_df_end.parquet\"\n",
    "vocab_df_end = pd.read_parquet(fp)\n",
    "fp = out_dir / \"kanji_df_end.parquet\"\n",
    "kanji_df_end = pd.read_parquet(fp)\n",
    "fp = out_dir / \"notes_df_end.parquet\"\n",
    "notes_df_end = pd.read_parquet(fp)\n",
    "\n",
    "vocab_idx = index_vocab_by_kanji(vocab_df_end)\n",
    "\n",
    "kanjidic2_df = parse_kanjidic2_to_df(data_dir / \"kanjidic2.xml\")\n",
    "jmdict_df = parse_jmdict_to_df(data_dir / \"JMdict_e.xml\", JMDICT_ENTITIES)\n",
    "pitch_df = parse_pitch_accents_to_df(data_dir / \"pitch_accents.txt\")\n",
    "\n",
    "_, vocab_used = build_final_kanji_notes(kanji_df_end, vocab_df_end, vocab_idx, top_n_vocab=top_n_vocab, mp3s_dir=mp3s_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87500f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.8)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
